import tkinter as tk
from tkinter import messagebox, ttk
import asyncio
import re
import os
import requests
import pandas as pd
from io import BytesIO
from PIL import Image
from playwright.async_api import async_playwright
from datetime import datetime
import threading
import sys

# ğŸ›‘ ì¤‘ë‹¨ ì‹ í˜¸ìš© ë³€ìˆ˜
stop_event = threading.Event()
work_popup = None

# --- GUI ì½˜ì†” ì¶œë ¥ ë¦¬ë‹¤ì´ë ‰íŠ¸ ---
class TextRedirector:
    def __init__(self, widget):
        self.widget = widget
    def write(self, str):
        self.widget.insert(tk.END, str)
        self.widget.see(tk.END)
    def flush(self):
        pass

# --- ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ ---
def clean_filename(filename):
    return re.sub(r'[\/:*?"<>|]', '', filename).strip()

def merge_detail_images(image_urls, save_path):
    if not image_urls: return False
    downloaded_images = []
    total_height, max_width = 0, 0
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    unique_urls = list(dict.fromkeys(image_urls))
    
    for url in unique_urls:
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code == 200:
                img = Image.open(BytesIO(resp.content))
                if img.width < 100 or img.height < 50: continue
                if img.mode in ("RGBA", "P"): img = img.convert("RGB")
                downloaded_images.append(img)
                total_height += img.height
                max_width = max(max_width, img.width)
        except: continue
        
    if not downloaded_images: return False
    canvas = Image.new('RGB', (max_width, total_height), (255, 255, 255))
    curr_y = 0
    for img in downloaded_images:
        canvas.paste(img, (0, curr_y))
        curr_y += img.height
    canvas.save(save_path, "JPEG", quality=85)
    return True

# --- ë§ˆì¼“ì»¬ë¦¬ í¬ë¡¤ë§ í•µì‹¬ ë¡œì§ ---
async def crawl_logic(urls, goal_count, collect_options):
    global work_popup
    async with async_playwright() as p:
        try:
            start_time_str = datetime.now().strftime("%Y%m%d_%H%M")
            base_folder = f"kurly_images_{start_time_str}"
            os.makedirs(base_folder, exist_ok=True)
            
            print(f"ğŸ”— ë¸Œë¼ìš°ì € ì‹¤í–‰... (í´ë”: {base_folder})")
            browser = await p.chromium.launch(headless=False) 
            context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
            page = await context.new_page()

            all_results = []
            
            for idx, url in enumerate(urls, 1):
                if not url.strip() or stop_event.is_set(): continue
                cat_folder = f"{base_folder}/{idx}ë²ˆì£¼ì†Œ"
                print(f"ğŸš€ [{idx}ë²ˆì£¼ì†Œ] ìˆ˜ì§‘ ì‹œì‘")
                
                await page.goto(url, wait_until="domcontentloaded")
                await asyncio.sleep(3)
                await page.mouse.wheel(0, 2000)
                await asyncio.sleep(1)

                items = await page.query_selector_all("a[href*='/goods/'], [data-testid='product-item']")
                
                success_count = 0
                processed_urls = set()

                for item in items:
                    if stop_event.is_set() or success_count >= goal_count: break 

                    try:
                        # [ê°€ê²© ì¶”ì¶œ] ì„±ê³µí–ˆë˜ ë¡œì§ ê·¸ëŒ€ë¡œ ì ìš©
                        price = 0
                        price_el = await item.query_selector("span[class*='1mlutp'], [class*='Price']")
                        if price_el:
                            p_text = await price_el.inner_text()
                            price = int(re.sub(r'[^0-9]', '', p_text))

                        raw_url = await item.get_attribute("href")
                        if not raw_url:
                            link_el = await item.query_selector("a")
                            raw_url = await link_el.get_attribute("href") if link_el else None
                        
                        if not raw_url: continue
                        detail_url = "https://www.kurly.com" + raw_url if raw_url.startswith("/") else raw_url
                        if detail_url in processed_urls: continue

                        # ìƒì„¸ í˜ì´ì§€ ì´ë™
                        detail_page = await context.new_page()
                        await detail_page.goto(detail_url)
                        await asyncio.sleep(2)

                        full_name_el = await detail_page.query_selector("h1")
                        full_name = (await full_name_el.inner_text()).strip() if full_name_el else "ì´ë¦„ì—†ìŒ"
                        clean_name = clean_filename(full_name)

                        # --- [ì¤‘ëŸ‰/ìˆ˜ëŸ‰ ê³„ì‚°: ë„¤ê°€ ì£¼ì‹  ë¡œì§ ê¸°ë°˜ ë³´ì •] ---
                        weight_match = re.search(r'(\d+(?:\.\d+)?)\s*(g|kg|ml|L|KG|Kg|kG|ML|Ml|mL)', full_name)
                        count_match = re.search(r'(\d+)\s*(ê°œ|íŒ©|ì…)', full_name)
                        final_weight = "ì§ì ‘í™•ì¸"

                        if weight_match and count_match:
                            try:
                                value = float(weight_match.group(1))
                                unit = weight_match.group(2).lower()
                                count_val = int(count_match.group(1))
                                total_value = value * count_val
                                
                                # ì†Œìˆ˜ì  ê¹”ë”í•˜ê²Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ :g í¬ë§· ì‚¬ìš©
                                if unit in ['kg', 'l']:
                                    final_weight = f"{total_value:g}{unit}"
                                elif unit == 'g':
                                    if total_value >= 1000: final_weight = f"{total_value/1000:g}kg"
                                    else: final_weight = f"{total_value:g}g"
                                elif unit == 'ml':
                                    if total_value >= 1000: final_weight = f"{total_value/1000:g}L"
                                    else: final_weight = f"{total_value:g}ml"
                            except:
                                final_weight = "ê³„ì‚°ì‹¤íŒ¨"
                        elif weight_match:
                            # ê°œìˆ˜ ì •ë³´ê°€ ì—†ì„ ë•ŒëŠ” ë§¤ì¹­ëœ ë¬¸ìì—´ ê·¸ëŒ€ë¡œ (ì˜ˆ: 2.5kg)
                            final_weight = weight_match.group(0).strip()

                        # --- [ë°ì´í„° ì €ì¥: ìš”ì²­í•œ ì—‘ì…€ ìˆœì„œ ì ìš©] ---
                        product_data = {}
                        if collect_options["ë¸Œëœë“œ"]:
                            brand_match = re.search(r'^\[(.*?)\]', full_name)
                            product_data["ë¸Œëœë“œëª…"] = brand_match.group(1) if brand_match else "ì»¬ë¦¬"
                        
                        if collect_options["ìƒí’ˆëª…"]: product_data["ìƒí’ˆëª…"] = full_name
                        
                        origin = "êµ­ë‚´ì‚°"
                        for o in ["í˜¸ì£¼", "ë¯¸êµ­", "ìºë‚˜ë‹¤", "ìŠ¤í˜ì¸", "ìˆ˜ì…", "ì¹ ë ˆ", "ë©•ì‹œì½”", "í•„ë¦¬í•€", "ë…¸ë¥´ì›¨ì´", "íƒœêµ­", "êµ­ë‚´ì‚°", "í˜ë£¨", "ë‰´ì§ˆëœë“œ"]:
                            if o in full_name: origin = o; break
                        if collect_options["ì›ì‚°ì§€"]: product_data["ì›ì‚°ì§€"] = origin
                        if collect_options["ì¤‘ëŸ‰/ìˆ˜ëŸ‰"]: product_data["ì¤‘ëŸ‰/ìˆ˜ëŸ‰"] = final_weight
                        if collect_options["ê°€ê²©"]: product_data["ê°€ê²©"] = price # ìˆ«ìë§Œ ì €ì¥
                        if collect_options["ìƒí’ˆURL"]: product_data["ìƒí’ˆURL"] = detail_url

                        # ì´ë¯¸ì§€ ì €ì¥ í´ë” ìƒì„± ë° ëŒ€í‘œì´ë¯¸ì§€ ì €ì¥
                        product_folder = f"{cat_folder}/{clean_name}"
                        os.makedirs(product_folder, exist_ok=True)

                        main_img = await detail_page.query_selector("img[alt*='ëŒ€í‘œ-ì´ë¯¸ì§€'], [class*='zjvv7'] img")
                        if main_img:
                            m_src = await main_img.get_attribute("src")
                            r = requests.get(m_src, timeout=10)
                            with open(f"{product_folder}/{clean_name}_ëŒ€í‘œì´ë¯¸ì§€.jpg", "wb") as f: f.write(r.content)

                        # [ìƒì„¸ì´ë¯¸ì§€ ìˆ˜ì§‘] #description + #detail í•©ì¹˜ê¸°
                        detail_urls = []
                        for selector in ["#description", "#detail"]:
                            container = await detail_page.query_selector(selector)
                            if container:
                                imgs = await container.query_selector_all("img")
                                detail_urls.extend([await i.get_attribute("src") for i in imgs if await i.get_attribute("src")])

                        if detail_urls:
                            merge_detail_images(detail_urls, f"{product_folder}/{clean_name}_ìƒì„¸ì´ë¯¸ì§€.jpg")

                        all_results.append(product_data)
                        processed_urls.add(detail_url)
                        success_count += 1
                        print(f"âœ… [{idx}ë²ˆ-{success_count}/{goal_count}] ì™„ë£Œ: {clean_name} | {final_weight} | {price}")
                        
                        await detail_page.close()

                    except Exception as e:
                        print(f"âš ï¸ ìŠ¤í‚µ: {e}")
                        continue
            
            if all_results:
                df = pd.DataFrame(all_results)
                # ìš”ì²­í•˜ì‹  ìˆœì„œëŒ€ë¡œ ì»¬ëŸ¼ ì¬ë°°ì¹˜
                order = ["ë¸Œëœë“œëª…", "ìƒí’ˆëª…", "ì›ì‚°ì§€", "ì¤‘ëŸ‰/ìˆ˜ëŸ‰", "ê°€ê²©", "ìƒí’ˆURL"]
                cols = [c for c in order if c in df.columns]
                df[cols].to_excel(f"ë§ˆì¼“ì»¬ë¦¬_ìˆ˜ì§‘ê²°ê³¼_{start_time_str}.xlsx", index=False)
                messagebox.showinfo("ì™„ë£Œ", f"ì´ {len(all_results)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ!")
            
            await browser.close()
        except Exception as e:
            print(f"âŒ ì—ëŸ¬: {e}")
        finally:
            if work_popup and work_popup.winfo_exists(): work_popup.destroy()

# --- GUI (ê¸°ì¡´ ìœ ì§€) ---
def start_thread():
    global work_popup
    stop_event.clear()
    sys.stdout = TextRedirector(log_text)
    urls = [entry.get() for entry in url_entries if entry.get().strip()]
    if not urls or not goal_entry.get(): return
    
    work_popup = tk.Toplevel(root); work_popup.title("ì‘ì—… ì¤‘"); work_popup.geometry("300x120")
    tk.Label(work_popup, text="ë§ˆì¼“ì»¬ë¦¬ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤...", font=('Arial', 10, 'bold'), pady=10).pack()
    tk.Button(work_popup, text="ì§€ê¸ˆ ì¤‘ë‹¨í•˜ê¸°", command=stop_crawling, bg="#f44336", fg="white", font=('Arial', 9, 'bold')).pack(pady=5)
    work_popup.grab_set()

    options = {"ë¸Œëœë“œ": var_brand.get(), "ìƒí’ˆëª…": var_name.get(), "ì›ì‚°ì§€": var_origin.get(), "ì¤‘ëŸ‰/ìˆ˜ëŸ‰": var_weight.get(), "ê°€ê²©": var_price.get(), "ìƒí’ˆURL": var_url.get()}
    threading.Thread(target=lambda: asyncio.run(crawl_logic(urls, int(goal_entry.get()), options)), daemon=True).start()

def stop_crawling():
    stop_event.set()
    if work_popup and work_popup.winfo_exists(): work_popup.destroy()

def add_url_entry():
    row = len(url_entries) + 1
    tk.Label(url_frame, text=f"{row}.").grid(row=row, column=0, padx=5)
    entry = tk.Entry(url_frame, width=70); entry.grid(row=row, column=1, pady=2)
    url_entries.append(entry)


root = tk.Tk(); root.title("ë§ˆì¼“ì»¬ë¦¬ ìˆ˜ì§‘ê¸° v2.0"); root.geometry("650x750")
goal_frame = tk.Frame(root); goal_frame.pack(pady=5)


tk.Label(goal_frame, text="ğŸ’¡ ì¹´í…Œê³ ë¦¬ë‹¹ ëª©í‘œ ê°œìˆ˜: ").pack(side=tk.LEFT)


goal_entry = tk.Entry(goal_frame, width=10); goal_entry.insert(0, "30"); goal_entry.pack(side=tk.LEFT, padx=5)
check_frame = tk.LabelFrame(root, text="ğŸ“Š ìˆ˜ì§‘ ë°ì´í„° ì„ íƒ", padx=10, pady=5); check_frame.pack(pady=5, padx=10, fill="x")
var_brand = tk.BooleanVar(value=True); var_name = tk.BooleanVar(value=True); var_origin = tk.BooleanVar(value=True)
var_weight = tk.BooleanVar(value=True); var_price = tk.BooleanVar(value=True); var_url = tk.BooleanVar(value=True)


tk.Checkbutton(check_frame, text="ë¸Œëœë“œ", variable=var_brand).grid(row=0, column=0, sticky="w")
tk.Checkbutton(check_frame, text="ìƒí’ˆëª…", variable=var_name).grid(row=0, column=1, sticky="w")
tk.Checkbutton(check_frame, text="ì›ì‚°ì§€", variable=var_origin).grid(row=0, column=2, sticky="w")
tk.Checkbutton(check_frame, text="ì¤‘ëŸ‰/ìˆ˜ëŸ‰", variable=var_weight).grid(row=1, column=0, sticky="w")
tk.Checkbutton(check_frame, text="ê°€ê²©", variable=var_price).grid(row=1, column=1, sticky="w")
tk.Checkbutton(check_frame, text="ìƒí’ˆURL", variable=var_url).grid(row=1, column=2, sticky="w")


url_frame = tk.LabelFrame(root, text="ğŸ”— ì»¬ë¦¬ ì¹´í…Œê³ ë¦¬ URL", padx=10, pady=5); url_frame.pack(pady=5, padx=10, fill="both", expand=True)
url_entries = []; add_url_entry()
btn_frame = tk.Frame(root); btn_frame.pack(pady=5)


tk.Button(btn_frame, text="â• URL ì¶”ê°€", command=add_url_entry).pack(side=tk.LEFT, padx=10)
tk.Button(btn_frame, text="ğŸš€ ìˆ˜ì§‘ ì‹œì‘", command=start_thread, bg="#5f0080", fg="white", font=('Arial', 10, 'bold'), width=15).pack(side=tk.LEFT, padx=10)
tk.Button(btn_frame, text="ğŸ›‘ ì¤‘ë‹¨", command=stop_crawling, bg="#f44336", fg="white", font=('Arial', 10, 'bold'), width=15).pack(side=tk.LEFT, padx=10)


log_frame = tk.LabelFrame(root, text="ğŸ“ ì‘ì—… í˜„í™©", padx=10, pady=5); log_frame.pack(pady=5, padx=10, fill="both", expand=True)
log_text = tk.Text(log_frame, height=8); log_text.pack(side=tk.LEFT, fill="both", expand=True)
log_scroll = tk.Scrollbar(log_frame, command=log_text.yview); log_scroll.pack(side=tk.RIGHT, fill="y")
log_text.configure(yscrollcommand=log_scroll.set)


root.mainloop()